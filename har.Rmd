---
output:
  html_document: default
  word_document: default
  pdf_document: default
---
Intro to Machine Learning: Final Project
==============================================================

##### Author: Sean Kalsi

### Executive Summary: 

This analysis will estimate the classification category for manner of barbell lift. The barbell lift activity data is collected using a Human Activity Recognition(har) device. Using a random forrest model we estimate an ~10% out of sample error for predicting the correct classification. 

Background info & links to datasets used:
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

### Data Partitioning:

To start with I split the training set into 3 parts: 1/4 for testing, 9/16 for training, 3/16 for validation.
```{R,message=FALSE,echo=FALSE,results="hide",warning=FALSE}
library(e1071)
library(caret)
library(dplyr)
```

```{R}
training1 <- read.csv("pml-training.csv")
testing1 <- read.csv("pml-testing.csv")

inTrain = createDataPartition(training1$classe, p = 3/4)[[1]]

trainingFull = training1[ inTrain,]
testing = training1[-inTrain,]

withinTrain = createDataPartition(trainingFull$classe, p = 3/4)[[1]]
training = trainingFull[ withinTrain,]
validation = trainingFull[-withinTrain,]

```

### Data Exploration:

To start with I examined the dataset to see how many of the 159 predictors were measured for all rows in the training set. It turns out 60 out of 159 contain no missing data. 

```{R}
len <- length(training)
lenVals <- length(training$classe)

nval <- 0
for(i in 1:len){
  if(class(training[,i]) == "factor"){
    nasum <- sum(training[,i] != "")
  }
  else nasum <- sum(!is.na(training[,i]))
  if(nasum == lenVals){
    if(nval<1){
      ind <- i
      vals <- nasum
      nval <- nval+1
    }
    else{
      ind <- c(ind,i)
      vals <- c(vals,nasum)
    } 
  }
}

```

Then I made plots of these 60 varriables and examined which variables showed the most distinct clustering. It appeared that after taking into account the user name some clear patterns emerged. The following variables appeared to have the strongest clustering: "roll_belt", "pitch_belt", "yaw_belt", "total_accel_belt", & "gyros_belt_x". As an example the plot below shows "pitch+belt" vs classe.

```{R,echo=FALSE}
ggplot(training,aes_string(x="classe",y=names(training[ind[9]])))+geom_point(aes(color = user_name))+labs(title = sprintf("var %d of %d", 9,length(ind)))+labs(y = names(training)[ind[9]], x = "Class", color = "user_name")

```

### Model Exploration:

I started by looking at models that are better for cluster searching given the output we were trying to predict was a factor variable. I compared the random forrest, tree search, naive bayes, and kmeans cluster models. The random forrest had the best accuracy with the training data, and had an out of sample accuracy of 90% on the validation set.

```{R,warning=FALSE}
outVar <- "classe"
subset <- names(training)[ind]
subset2 <- subset[c(8,9,10,11,12)]
predVars <- paste(subset2,collapse = " + ")
formula <- as.formula(paste(outVar,predVars,sep = " ~ "))

actTrain <- training$classe
actValid <- validation$classe

modelNames <- c("rf","rpart","nb","knn")

acTrain <- vector(length = length(modelNames),mode="numeric")
acValidation <- vector(length = length(modelNames),mode="numeric")

for(i in 1:length(modelNames)){
  
  mod <- train(formula, method=modelNames[i], data=training)
  
  pred <- predict(mod,training)
  acTrain[i] <- sum(pred==actTrain)/length(actTrain)
  pred <- predict(mod,validation)
  acValidation[i] <- sum(pred==actValid)/length(actValid)
}
data.frame(modelNames,acTrain,acValidation)

```

The Random Forrest model seemed to produce the best accuracy on the validation set. The next step was to cross validate this model. This cross validation tests had an average out of sample error of ~11% which was consistent with what this model predicted on the validation set.

```{R}

numCross  <- 4
acCrossTrain <- vector(length = numCross,mode="numeric")
acCrossTest <- vector(length = numCross,mode="numeric")

for(i in 1:numCross){
  inCross = createDataPartition(training$classe, p = 3/4)[[1]]
  trainCross = training[ inCross,]
  testCross = training[ -inCross,]
  actTrainCross <- trainCross$classe
  actTestCross <- testCross$classe
  mod <- train(formula, method="rf", data=trainCross)
  pred <- predict(mod,trainCross)
  acCrossTrain[i] <- sum(pred==actTrainCross)/length(actTrainCross)
  pred <- predict(mod,testCross)
  acCrossTest[i] <- sum(pred==actTestCross)/length(actTestCross)
}
data.frame(acCrossTrain,acCrossTest)
1-mean(acCrossTest)
```

### Results

The Random Forrest Model produced a 11% out of sample error on the test set, which was consisitent with the cross validation, and validation sets.

```{R}
actTest <- testing$classe
mod <- train(formula, method="rf",data=training)
pred <- predict(mod,testing)
predVal <- predict(mod,validation)
1-sum(pred==actTest)/length(actTest)
```

### Appendix

Here is the confusion Matrix for the Validation data

```{R}
confusionMatrix(predVal,actValid)
```

Here is the confusion Matrix for the Test data

```{R}
confusionMatrix(pred,actTest)
```
